# -*- coding: utf-8 -*-
"""sm of ISUKD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I_1QDVQOIeflNPA1uEGYbMZBNFsPHyZ6

## **1. Environment Setup**
"""

pip install torchmetrics

import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torch.utils.data import Dataset, DataLoader
from torchmetrics import StructuralSimilarityIndexMeasure as SSIM
from torchmetrics import PeakSignalNoiseRatio as PSNR
from torch.cuda.amp import GradScaler, autocast
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tqdm import tqdm

# Install dependencies (Colab)
def setup_environment():
    """Install required packages"""
    !pip install -q torch torchvision torchmetrics opencv-python matplotlib numpy tqdm
    !pip install -q git+https://github.com/swz30/Restormer.git

# Verify hardware
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
print(f"PyTorch version: {torch.__version__}")

"""## **2. Dataset Preparation**

```
Whole Dataset (100%)
├── 90% (Training + Validation)
│   ├── 80% → Training (72% of total)
│   └── 20% → Validation (18% of total)
└── 10% → Benchmark Testing
```
"""

class VideoConferencingDataset(Dataset):
    def __init__(self, sharp_dir, mode='train', img_size=256,
                 test_size=0.1, val_size=0.2, seed=42, augment=True, degradation='bicubic'):
        """
        Args:
            augment: Enable/disable data augmentation
            degradation: 'bicubic' or 'bilinear' for compression simulation
            (Other args remain the same)
        """
        self.img_size = img_size
        self.augment = augment  # Add this line
        self.degradation = degradation  # Add this line

        all_paths = sorted([
            os.path.join(sharp_dir, f) for f in os.listdir(sharp_dir)
            if f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])

        # First split: 90% train+val, 10% test (benchmark)
        trainval_paths, test_paths = train_test_split(
            all_paths, test_size=test_size, random_state=seed)

        # Second split: 80% train, 20% val (of the 90%)
        train_paths, val_paths = train_test_split(
            trainval_paths, test_size=val_size, random_state=seed)

        self.paths = {
            'train': train_paths,
            'val': val_paths,
            'test': test_paths
        }[mode]

    def __len__(self):
        return len(self.paths)

    def _simulate_compression(self, img):
        """Professional video compression simulation"""
        h, w = img.shape[:2]

        # 1. Downscale
        downscale_method = cv2.INTER_CUBIC if self.degradation == 'bicubic' else cv2.INTER_LINEAR
        downscaled = cv2.resize(img, (w//4, h//4), interpolation=downscale_method)

        # 2. Upscale with compression artifacts
        upscaled = cv2.resize(downscaled, (w, h), interpolation=cv2.INTER_CUBIC)

        # 3. Add JPEG-like artifacts
        _, enc_img = cv2.imencode('.jpg', upscaled, [int(cv2.IMWRITE_JPEG_QUALITY), 75])
        compressed = cv2.imdecode(enc_img, 1)

        return compressed

    def _augment(self, img):
        """Professional augmentation pipeline"""
        # Random horizontal flip
        if np.random.rand() > 0.5:
            img = cv2.flip(img, 1)

        # Random rotation (-15 to +15 degrees)
        angle = np.random.uniform(-15, 15)
        h, w = img.shape[:2]
        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1)
        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)

        # Color jitter (simulate WB changes)
        if np.random.rand() > 0.5:
            img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
            img[:,:,1] = np.clip(img[:,:,1] * np.random.uniform(0.8, 1.2), 0, 255)
            img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)

        return img

    def __getitem__(self, idx):
        # Load and convert image
        sharp_img = cv2.imread(self.paths[idx])
        sharp_img = cv2.cvtColor(sharp_img, cv2.COLOR_BGR2RGB)

        # Generate blurry version
        blurry_img = self._simulate_compression(sharp_img)

        # Apply augmentations
        if self.augment:
            sharp_img = self._augment(sharp_img)
            blurry_img = self._augment(blurry_img)

        # Random crop
        if self.img_size:
            h, w = sharp_img.shape[:2]
            i = np.random.randint(0, h - self.img_size) if self.augment else (h - self.img_size) // 2
            j = np.random.randint(0, w - self.img_size) if self.augment else (w - self.img_size) // 2
            sharp_img = sharp_img[i:i+self.img_size, j:j+self.img_size]
            blurry_img = blurry_img[i:i+self.img_size, j:j+self.img_size]

        # Convert to tensor
        sharp_tensor = torch.from_numpy(sharp_img).permute(2,0,1).float() / 255.0
        blurry_tensor = torch.from_numpy(blurry_img).permute(2,0,1).float() / 255.0

        return blurry_tensor, sharp_tensor

# # Example usage (replace with your dataset path)
# train_dataset = SharpeningDataset("/path/to/sharp_images", img_size=256)
# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

"""## **3. Model Architecture**

### **A. Teacher Model (Restormer-based)**
"""

class ImageRestorationTeacher(nn.Module):
    """High-performance teacher model for image sharpening"""
    def __init__(self):
        super().__init__()
        # Simplified Restormer architecture
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.GELU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.GELU(),
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.GELU()
        )

        # Transformer blocks (simplified)
        self.transformer = nn.Sequential(
            *[TransformerBlock(256) for _ in range(4)]
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.GELU(),
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.GELU(),
            nn.Conv2d(64, 3, kernel_size=3, padding=1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.transformer(x)
        x = self.decoder(x)
        return torch.sigmoid(x)

class TransformerBlock(nn.Module):
    """Simplified transformer block for teacher model"""
    def __init__(self, dim):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(dim, num_heads=4)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim*4),
            nn.GELU(),
            nn.Linear(dim*4, dim)
        )
    def forward(self, x):
        B, C, H, W = x.shape
        x = x.flatten(2).permute(2, 0, 1)  # (H*W, B, C)
        x = x + self.attn(self.norm(x), self.norm(x), self.norm(x))[0]
        x = x + self.mlp(self.norm(x))
        return x.permute(1, 2, 0).view(B, C, H, W)

"""### **B. Student Model (Lightweight)**"""

class VideoSharpeningStudent(nn.Module):
    """
    Ultra-Lightweight Image Sharpening Model
    Key Features:
    - <1M parameters
    - Depthwise separable convolutions
    - Channel attention
    - PixelShuffle upsampling
    - Residual connections
    """

    def __init__(self):
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(0.2),

            nn.Conv2d(32, 32, 3, padding=1, groups=32, bias=False),
            nn.Conv2d(32, 64, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),

            nn.Conv2d(64, 64, 3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2)
        )

        # Attention
        self.attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(64, 64//8, 1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64//8, 64, 1),
            nn.Sigmoid()
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Conv2d(64, 256, 3, padding=1),
            nn.PixelShuffle(2),
            nn.Conv2d(64, 3, 3, padding=1)
        )

        # Skip connection
        self.skip = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2)
        )

    def forward(self, x):
        residual = self.skip(x)
        x = self.encoder(x)
        x = x * self.attention(x)
        x = self.decoder(x)
        return torch.sigmoid(x + residual[:, :x.shape[1], :, :])

"""## **4. Loss Functions**"""

class PerceptualLoss(nn.Module):
    """VGG16-based perceptual loss"""
    def __init__(self):
        super().__init__()
        vgg = torchvision.models.vgg16(pretrained=True).features[:16]
        self.vgg = vgg.eval().to(device)
        for param in self.vgg.parameters():
            param.requires_grad = False

    def forward(self, input, target):
        input_features = self.vgg(input)
        target_features = self.vgg(target)
        return F.l1_loss(input_features, target_features)

class EdgeLoss(nn.Module):
    """Sobel-based edge preservation loss (fixed for RGB inputs)"""
    def __init__(self):
        super().__init__()
        kernel = torch.tensor([[-1,-1,-1], [-1,8,-1], [-1,-1,-1]],
                            dtype=torch.float32).view(1,1,3,3)
        self.register_buffer('kernel', kernel.repeat(3,1,1,1).to(device))  # Changed for RGB

    def forward(self, input, target):
        # Process each channel separately
        input_edges = F.conv2d(input, self.kernel, padding=1, groups=3)  # Added groups=3
        target_edges = F.conv2d(target, self.kernel, padding=1, groups=3)
        return F.l1_loss(input_edges, target_edges)

"""## **5. Training Pipeline**"""

class Trainer:
    """Professional training pipeline with:
    - Mixed precision training
    - Learning rate scheduling
    - Model checkpointing
    - Progress visualization
    - Automatic device handling
    """

    def __init__(self, student, teacher, train_loader, val_loader):
        self.student = student
        self.teacher = teacher
        self.train_loader = train_loader
        self.val_loader = val_loader

        # Initialize components with proper device handling
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.l1_loss = nn.L1Loss().to(self.device)
        self.perceptual_loss = PerceptualLoss().to(self.device)
        self.edge_loss = EdgeLoss().to(self.device)

        # Optimizer with gradient clipping
        self.optimizer = torch.optim.AdamW(
            student.parameters(),
            lr=1e-4,
            weight_decay=1e-4
        )

        # OneCycleLR scheduler with warmup
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=1e-3,
            steps_per_epoch=len(train_loader),
            epochs=100,
            pct_start=0.3
        )

        # AMP components
        self.scaler = torch.amp.GradScaler(enabled=(self.device.type == 'cuda'))
        self.autocast = torch.amp.autocast(
            device_type=self.device.type,
            dtype=torch.float16 if self.device.type == 'cuda' else torch.bfloat16
        )

        # Training tracking
        self.best_metrics = {
            'ssim': 0,
            'psnr': 0,
            'epoch': -1
        }
        self.history = {
            'train_loss': [],
            'val_ssim': [],
            'val_psnr': [],
            'lr': []
        }

    def train_epoch(self, epoch):
        self.student.train()
        epoch_loss = 0
        progress = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")

        for batch_idx, (blurry, sharp) in enumerate(progress):
            blurry = blurry.to(self.device, non_blocking=True)
            sharp = sharp.to(self.device, non_blocking=True)

            with self.autocast:
                # Teacher guidance
                with torch.no_grad():
                    teacher_out = self.teacher(blurry)

                # Student prediction
                student_out = self.student(blurry)

                # Loss composition
                loss = (
                    1.0 * self.l1_loss(student_out, sharp) +
                    0.5 * self.perceptual_loss(student_out, sharp) +
                    0.2 * self.edge_loss(student_out, sharp) +
                    0.1 * F.mse_loss(student_out, teacher_out)  # Distillation
                )

            # Backpropagation
            self.optimizer.zero_grad(set_to_none=True)
            self.scaler.scale(loss).backward()

            # Gradient clipping
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.student.parameters(), 1.0)

            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.scheduler.step()

            # Update tracking
            epoch_loss += loss.item()
            self.history['lr'].append(self.optimizer.param_groups[0]['lr'])
            progress.set_postfix({
                'loss': f"{loss.item():.4f}",
                'lr': f"{self.history['lr'][-1]:.2e}"
            })

        avg_loss = epoch_loss / len(self.train_loader)
        self.history['train_loss'].append(avg_loss)
        return avg_loss

    @torch.no_grad()
    def validate(self):
        self.student.eval()
        ssim = SSIM().to(self.device)
        psnr = PSNR().to(self.device)

        for blurry, sharp in self.val_loader:
            blurry = blurry.to(self.device, non_blocking=True)
            sharp = sharp.to(self.device, non_blocking=True)

            with self.autocast:
                output = self.student(blurry)
                ssim.update(output, sharp)
                psnr.update(output, sharp)

        val_ssim = ssim.compute().item()
        val_psnr = psnr.compute().item()

        self.history['val_ssim'].append(val_ssim)
        self.history['val_psnr'].append(val_psnr)

        return val_ssim, val_psnr

    def save_checkpoint(self, epoch, val_ssim, val_psnr):
        if val_ssim > self.best_metrics['ssim']:
            self.best_metrics.update({
                'ssim': val_ssim,
                'psnr': val_psnr,
                'epoch': epoch
            })

            torch.save({
                'epoch': epoch,
                'model_state': self.student.state_dict(),
                'optimizer_state': self.optimizer.state_dict(),
                'scaler_state': self.scaler.state_dict(),
                'metrics': {
                    'ssim': val_ssim,
                    'psnr': val_psnr
                },
                'history': self.history
            }, f"best_checkpoint_epoch{epoch}.pth")

    def train(self, epochs=100):
        try:
            for epoch in range(epochs):
                train_loss = self.train_epoch(epoch)
                val_ssim, val_psnr = self.validate()

                print(f"\nEpoch {epoch+1}/{epochs} | "
                      f"Train Loss: {train_loss:.4f} | "
                      f"Val SSIM: {val_ssim:.4f} | "
                      f"Val PSNR: {val_psnr:.2f} dB | "
                      f"LR: {self.history['lr'][-1]:.2e}")

                self.save_checkpoint(epoch, val_ssim, val_psnr)

        except KeyboardInterrupt:
            print("\nTraining interrupted - saving current state...")
            self.save_checkpoint(epoch, val_ssim, val_psnr)

        print(f"\nBest Model: Epoch {self.best_metrics['epoch']+1} | "
              f"SSIM: {self.best_metrics['ssim']:.4f} | "
              f"PSNR: {self.best_metrics['psnr']:.2f} dB")

"""## **6. Evaluation and Utils**"""

def visualize_results(model, loader, num_samples=3):
    """Professional quality visualization"""
    model.eval()
    plt.figure(figsize=(15, 5*num_samples))

    with torch.no_grad():
        for i, (blurry, sharp) in enumerate(loader):
            if i >= num_samples:
                break

            blurry, sharp = blurry.to(device), sharp.to(device)
            output = model(blurry)

            # Convert to numpy
            blurry_np = blurry[0].cpu().permute(1,2,0).numpy()
            output_np = output[0].cpu().permute(1,2,0).numpy()
            sharp_np = sharp[0].cpu().permute(1,2,0).numpy()

            # Plot
            plt.subplot(num_samples, 3, i*3+1)
            plt.imshow(blurry_np)
            plt.title("Blurry Input")
            plt.axis('off')

            plt.subplot(num_samples, 3, i*3+2)
            plt.imshow(output_np)
            plt.title("Sharpened Output")
            plt.axis('off')

            plt.subplot(num_samples, 3, i*3+3)
            plt.imshow(sharp_np)
            plt.title("Ground Truth")
            plt.axis('off')

    plt.tight_layout()
    plt.show()

def benchmark_inference(model, resolution=(1920, 1080), repetitions=100):
    """Professional benchmarking tool"""
    dummy_input = torch.randn(1, 3, *resolution).to(device)

    # Warmup
    for _ in range(10):
        _ = model(dummy_input)

    # Timed inference
    starter = torch.cuda.Event(enable_timing=True)
    ender = torch.cuda.Event(enable_timing=True)

    torch.cuda.synchronize()
    starter.record()
    for _ in range(repetitions):
        _ = model(dummy_input)
    ender.record()
    torch.cuda.synchronize()

    avg_time = starter.elapsed_time(ender) / repetitions
    fps = 1000 / avg_time

    print(f"Inference Benchmark @ {resolution[0]}x{resolution[1]}:")
    print(f"- Average latency: {avg_time:.2f}ms")
    print(f"- Throughput: {fps:.1f} FPS")
    print(f"- VRAM usage: {torch.cuda.max_memory_allocated()/1e6:.1f}MB")

    return avg_time

"""## **7. Main Execution**

**Dataset Preparation:**
"""

# Download sample dataset
!wget https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip
!unzip DIV2K_train_HR.zip -d /content/DIV2K

"""**Training Execution:**"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from models.student_model import VideoSharpeningStudent

# ---- Dataset ----
class PairedImageDataset(Dataset):
    def __init__(self, blurry_dir, sharp_dir, img_size=256):
        self.blurry_paths = sorted([os.path.join(blurry_dir, f) for f in os.listdir(blurry_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        self.sharp_paths = sorted([os.path.join(sharp_dir, f) for f in os.listdir(sharp_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor()
        ])
    def __len__(self):
        return min(len(self.blurry_paths), len(self.sharp_paths))
    def __getitem__(self, idx):
        blurry = Image.open(self.blurry_paths[idx]).convert('RGB')
        sharp = Image.open(self.sharp_paths[idx]).convert('RGB')
        return self.transform(blurry), self.transform(sharp)

# ---- Model Setup ----
device = torch.device("cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu"))
print("Using device:", device)

# Load pretrained teacher (Restormer)
teacher = torch.hub.load('swz30/Restormer', 'Restormer').eval().to(device)
for p in teacher.parameters():
    p.requires_grad = False

# Load student (your lightweight model)
student = VideoSharpeningStudent().to(device)

# ---- Training Setup ----
train_dataset = PairedImageDataset('data/blurry', 'data/sharp', img_size=256)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)

optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)
l1_loss = nn.L1Loss()
mse_loss = nn.MSELoss()

# ---- Training Loop ----
epochs = 10  # Adjust as needed
for epoch in range(epochs):
    student.train()
    running_loss = 0.0
    for blurry, sharp in train_loader:
        blurry, sharp = blurry.to(device), sharp.to(device)
        with torch.no_grad():
            teacher_out = teacher(blurry)
        student_out = student(blurry)
        loss = (
            1.0 * l1_loss(student_out, sharp) +
            0.1 * mse_loss(student_out, teacher_out)
        )
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(train_loader):.4f}")

# ---- Save Weights ----
os.makedirs('models', exist_ok=True)
torch.save(student.state_dict(), 'models/student_model.pth')
print("Training complete. Student weights saved to models/student_model.pth")

"""**CHECK**"""



"""## **7. Deployment Preparation**"""

# Export model
# export_to_onnx(student)

"""## **8. Performance Benchmarking**"""

# Example benchmark:
# benchmark_inference(student)